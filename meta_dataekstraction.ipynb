{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import json\n",
    "import glob\n",
    "from numpyencoder import NumpyEncoder\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"Dataset\"\n",
    "METADATA= glob.glob(f\"{PATH}/**/**.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_meta_data(file_name,dict_file):\n",
    "    with open(f'{file_name}.json', 'w') as fp:\n",
    "        json.dump(dict_file, fp,  indent=4,cls=NumpyEncoder)\n",
    "\n",
    "def read_meta_data (path):\n",
    "    with open(path, 'r') as j:\n",
    "        contents = json.loads(j.read())\n",
    "        return contents\n",
    "        \n",
    "def del_data(files_name,folder_name=\"Dataset\"):\n",
    "    try:\n",
    "        for d in files_name:\n",
    "            os.remove(f\"{folder_name}/{d}\")\n",
    "    except:\n",
    "        os.remove(f\"{folder_name}/{files_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "buff = glob.glob(f\"{PATH}/T_316000_233500_NE_T_316000_233500_SW/**.txt\")\n",
    "data = np.array([i.split(\"/\") for i in buff if \"building\" in i or \"buidling\" in i])\n",
    "new= data[:,-1].copy()\n",
    "new = [i.split(\"_\") for i in new]\n",
    "for i,_ in enumerate(new):\n",
    "    if len(new[i])==2:\n",
    "        new[i][1] = new[i][1].split(\".\")\n",
    "        new[i][1][0]= new[i][1][0].zfill(3)\n",
    "        new[i][1]= \".\".join(new[i][1])\n",
    "    else:\n",
    "        new[i][1]=new[i][1].zfill(3)\n",
    "    new[i]= \"_\".join(new[i])\n",
    "index = np.where(data[:,-1]!=new)[0]\n",
    "if index.size != 0:\n",
    "    old_data= data[:,-1]\n",
    "    METADATA_buff = read_meta_data(f'{PATH}/T_316000_233500_NE_T_316000_233500_SW/meta_data.json')\n",
    "    OLD_METADATA_buff = METADATA_buff.copy()\n",
    "    for i in index:\n",
    "        try:\n",
    "            data_edited= METADATA_buff.pop(old_data[i][:-4])\n",
    "        except:\n",
    "            continue\n",
    "        list_new_data=new[i].split(\"/\")\n",
    "        METADATA_buff[list_new_data[-1][:-4]]={\n",
    "                                                \"path\":f\"T_316000_233500_NE_T_316000_233500_SW/{new[i]}\",\n",
    "                                                \"jumlah_point\":data_edited[\"jumlah_point\"]\n",
    "                                                }\n",
    "\n",
    "    print(index)\n",
    "    save_meta_data(f'{PATH}/T_316000_233500_NE_T_316000_233500_SW/meta_data',METADATA_buff)\n",
    "    for idx in index:\n",
    "        os.rename(f\"{PATH}/T_316000_233500_NE_T_316000_233500_SW/{old_data[idx]}\",f\"{PATH}/T_316000_233500_NE_T_316000_233500_SW/{new[idx]}\")\n",
    "    #     # print()\n",
    "    #     # print(old_data)\n",
    "METADATA= glob.glob(f\"{PATH}/**/**.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 14.86it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_list_per_instace(str_name ,file_list,Area,dict_file):\n",
    "    result_dict = {}\n",
    "    id_ns = np.unique(np.array([b.split(\"_\")[1] for b in dict_file],dtype=\"int16\"))\n",
    "    zf = 3 if Area ==  \"T_316000_233500_NE_T_316000_233500_SW\" or Area==\"T_315500_234500_SE\" else 2\n",
    "    for idn in id_ns :\n",
    "        r = re.compile(f\"{str_name}_{str(idn).zfill(zf)}\")\n",
    "        result_dict[f\"{str_name}_{str(idn).zfill(zf)}\"] = list(filter(r.match, file_list)) \n",
    "    return result_dict\n",
    "\n",
    "for i,_ in tqdm.tqdm(enumerate(METADATA),total=len(METADATA)):\n",
    "    Area = METADATA[i].split(\"/\")[1]\n",
    "    md = read_meta_data(METADATA[i])\n",
    "    file_names = md.keys()\n",
    "    building,ground,undefined,vegetation=[],[],[],[]\n",
    "    label = set([i.split(\"_\")[0] for i in file_names])\n",
    "    for fn in file_names:\n",
    "        if 'building' in fn.lower() or \"buidling\" in fn.lower():\n",
    "            building.append(fn)\n",
    "        if 'ground' in fn.lower():\n",
    "            ground.append(fn)\n",
    "        if 'undefined' in fn.lower() or \"undifined\" in fn.lower() or \"undefined\" in fn.lower() or 'Undefined' in fn.lower():\n",
    "            undefined.append(fn)\n",
    "        if 'vegetation'in fn.lower():\n",
    "            vegetation.append(fn)\n",
    "    dict_area={\n",
    "        \"building\":get_list_per_instace(str_name = \"building\",file_list = building,dict_file=building,Area=Area),\n",
    "        \"vegetation\":vegetation,\n",
    "        \"ground\":ground,\n",
    "        \"undefined\":undefined\n",
    "        }\n",
    "    save_meta_data(f\"map_ins_{Area}\",dict_area)\n",
    "\n",
    "def maping_meta_data(class_name,map_data,meta_data):\n",
    "    inst_id = {}\n",
    "    try:\n",
    "        for count,key in enumerate(map_data[class_name]):\n",
    "            try:\n",
    "                k_split = key.split(\"_\")\n",
    "                if len(k_split)==3:\n",
    "                    _,sub_class,key_id = k_split\n",
    "                else :\n",
    "                    sub_class,key_id[:-4] = k_split\n",
    "            except:\n",
    "                sub_class,key_id=key,0\n",
    "                \n",
    "            inst_id[str(count).zfill(3)]={\n",
    "                \"total_point\": meta_data[key][\"jumlah_point\"],\n",
    "                \"path\": meta_data[key][\"path\"],\n",
    "                \"sub_class\": sub_class,\n",
    "                \"id\": str(int(key_id)-1).zfill(3)\n",
    "                }\n",
    "    except Exception as e:\n",
    "        print(class_name)\n",
    "        print(meta_data[class_name])\n",
    "        inst_id[\"000\"]={\n",
    "            \"total_point\": meta_data[class_name][\"jumlah_point\"],\n",
    "            \"path\": meta_data[class_name][\"path\"],\n",
    "            \"sub_class\": \"000\",\n",
    "            \"id\": \"000\"\n",
    "            }\n",
    "    return inst_id\n",
    "    \n",
    "def map_to_inst(map_data_json,meta_data_json):\n",
    "    map_data = read_meta_data(map_data_json)\n",
    "    meta_data= read_meta_data(meta_data_json)\n",
    "    for k in map_data.keys():\n",
    "        if k == \"building\":\n",
    "            inst_id_building = {}\n",
    "            list_build_id= list(map_data[k].keys())\n",
    "            for id_b in list_build_id:\n",
    "                id_ins = str(int(id_b.split(\"_\")[-1])-1).zfill(3)\n",
    "                total_point = sum([meta_data[i][\"jumlah_point\"] for i in map_data[list(map_data.keys())[0]][id_b]])\n",
    "                list_all_txt = [meta_data[i][\"path\"] for i in map_data[list(map_data.keys())[0]][id_b]]\n",
    "                inst_id_building[id_ins]={\n",
    "                    \"total_point\":total_point,\n",
    "                    \"path\":list_all_txt\n",
    "                    }\n",
    "        elif k==\"vegetation\" :\n",
    "            inst_id_vegetation = maping_meta_data(\"vegetation\",map_data,meta_data)\n",
    "        elif k==\"ground\" :\n",
    "            inst_id_ground = maping_meta_data(\"ground\",map_data,meta_data)\n",
    "        else :\n",
    "            inst_id_undefined = maping_meta_data(\"undefined\",map_data,meta_data)\n",
    "    return [inst_id_building,inst_id_vegetation,inst_id_ground,inst_id_undefined]\n",
    "\n",
    "for i,_ in enumerate(METADATA):\n",
    "    area=METADATA[i].split(\"/\")[1]\n",
    "    ins = map_to_inst(f\"map_ins_{area}.json\",METADATA[i])\n",
    "    save_meta_data(f\"map_ins_{area}\",{\"building\":ins[0],\"vegetation\":ins[1],\"ground\":ins[2],'undefined':ins[3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 12/13 [42:24<03:32, 212.08s/it] \n"
     ]
    }
   ],
   "source": [
    "MAPTOINST_META_LIST= glob.glob(\"map_**\")\n",
    "def get_combining_building_data(map_data_json = MAPTOINST_META_LIST,buff_folder=\"Dataset\") :   \n",
    "    for i,meta_name in tqdm.tqdm(enumerate(map_data_json),total=13):\n",
    "        map_data = read_meta_data(map_data_json[i])[\"building\"]\n",
    "        buff_folder = f\"all_data_building/{map_data_json[i][8:-5]}\"\n",
    "        os.makedirs(buff_folder,exist_ok=True)\n",
    "        map_data_id= list(map_data.keys())\n",
    "        for mdi in map_data_id:\n",
    "            list_data_txt = map_data[mdi][\"path\"]\n",
    "            # Start from init = 0 and append until last\n",
    "            all_data = np.loadtxt(f\"Dataset/{list_data_txt[0]}\")[:,:6]\n",
    "            for idx in range(1,len(list_data_txt)):\n",
    "                try:\n",
    "                    buff = np.loadtxt(f\"Dataset/{list_data_txt[idx]}\")[:,:6]\n",
    "                    all_data = np.append(all_data,buff,axis=0)\n",
    "                except :\n",
    "                    buff = np.loadtxt(f\"Dataset/{list_data_txt[idx]}\")[:6]\n",
    "                    all_data = np.append(all_data,[buff],axis=0)\n",
    "            del_data(list_data_txt)\n",
    "            if \"T_316000_233500_NE_T_316000_233500_SW\" in meta_name:\n",
    "                data_building_index = np.where(all_data[:,0]<318000)\n",
    "                data_bulding_sub_building_index = np.where(all_data[:,0]>318000)\n",
    "                data_building = all_data[data_building_index]\n",
    "                data_bulding_sub_building = all_data[data_bulding_sub_building_index]\n",
    "                data_bulding_sub_building[:,0] = data_bulding_sub_building[:,0]-2000\n",
    "                data_bulding_sub_building[:,1] = data_bulding_sub_building[:,1]-3619\n",
    "                data_bulding_sub_building[:,2] = data_bulding_sub_building[:,2]+40\n",
    "                all_data = np.append(data_building,data_bulding_sub_building,axis=0)\n",
    "            save_npy_loc = f\"{buff_folder}/building_{mdi}.npy\"\n",
    "            map_data[mdi][\"path\"] = save_npy_loc\n",
    "            np.save(save_npy_loc,all_data,allow_pickle=True)\n",
    "        old_map = read_meta_data(map_data_json[i])\n",
    "        old_map[\"building\"]= map_data\n",
    "        save_meta_data(f\"{map_data_json[i][8:-5]}\",old_map)\n",
    "        os.remove(f\"{map_data_json[i]}\")\n",
    "get_combining_building_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'map_ins_T_316000_234000_SW.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/media/nmhlog/BackUp Windows/GOINGTOTHESIS/meta_dataekstraction.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/nmhlog/BackUp%20Windows/GOINGTOTHESIS/meta_dataekstraction.ipynb#ch0000008?line=0'>1</a>\u001b[0m get_combining_building_data(map_data_json\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mmap_ins_T_316000_234000_SW.json\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[1;32m/media/nmhlog/BackUp Windows/GOINGTOTHESIS/meta_dataekstraction.ipynb Cell 6'\u001b[0m in \u001b[0;36mget_combining_building_data\u001b[0;34m(map_data_json, buff_folder)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/nmhlog/BackUp%20Windows/GOINGTOTHESIS/meta_dataekstraction.ipynb#ch0000006?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_combining_building_data\u001b[39m(map_data_json \u001b[39m=\u001b[39m MAPTOINST_META_LIST,buff_folder\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mall_data_building\u001b[39m\u001b[39m\"\u001b[39m) :   \n\u001b[1;32m      <a href='vscode-notebook-cell:/media/nmhlog/BackUp%20Windows/GOINGTOTHESIS/meta_dataekstraction.ipynb#ch0000006?line=2'>3</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i,meta_name \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(\u001b[39menumerate\u001b[39m(map_data_json),total\u001b[39m=\u001b[39m\u001b[39m13\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/nmhlog/BackUp%20Windows/GOINGTOTHESIS/meta_dataekstraction.ipynb#ch0000006?line=3'>4</a>\u001b[0m         map_data \u001b[39m=\u001b[39m read_meta_data(map_data_json[i])[\u001b[39m\"\u001b[39m\u001b[39mbuilding\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/nmhlog/BackUp%20Windows/GOINGTOTHESIS/meta_dataekstraction.ipynb#ch0000006?line=4'>5</a>\u001b[0m         buff_folder \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mall_data_building/\u001b[39m\u001b[39m{\u001b[39;00mmap_data_json[i][\u001b[39m8\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m5\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/nmhlog/BackUp%20Windows/GOINGTOTHESIS/meta_dataekstraction.ipynb#ch0000006?line=5'>6</a>\u001b[0m         os\u001b[39m.\u001b[39mmakedirs(buff_folder,exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32m/media/nmhlog/BackUp Windows/GOINGTOTHESIS/meta_dataekstraction.ipynb Cell 3'\u001b[0m in \u001b[0;36mread_meta_data\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/nmhlog/BackUp%20Windows/GOINGTOTHESIS/meta_dataekstraction.ipynb#ch0000002?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_meta_data\u001b[39m (path):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/nmhlog/BackUp%20Windows/GOINGTOTHESIS/meta_dataekstraction.ipynb#ch0000002?line=5'>6</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(path, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m j:\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/nmhlog/BackUp%20Windows/GOINGTOTHESIS/meta_dataekstraction.ipynb#ch0000002?line=6'>7</a>\u001b[0m         contents \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(j\u001b[39m.\u001b[39mread())\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/nmhlog/BackUp%20Windows/GOINGTOTHESIS/meta_dataekstraction.ipynb#ch0000002?line=7'>8</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m contents\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'map_ins_T_316000_234000_SW.json'"
     ]
    }
   ],
   "source": [
    "get_combining_building_data(map_data_json=[\"map_ins_T_316000_234000_SW.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['map_ins_T_316000_233500_NE_T_316000_233500_SW.json',\n",
       " 'map_ins_T_316000_233500_NW.json',\n",
       " 'map_ins_T_316000_233500_SE.json',\n",
       " 'map_ins_T_316000_234000_NE.json',\n",
       " 'map_ins_T_316000_234000_NW.json',\n",
       " 'map_ins_T_316000_234000_SE.json',\n",
       " 'map_ins_T_316000_234000_SW.json',\n",
       " 'map_ins_T_316500_234000_SW_T_316500_233500_NW.json',\n",
       " 'map_ins_T_315500_234500_SE.json',\n",
       " 'map_ins_T_315500_233500_NE_T_315500_234000_SE.json',\n",
       " 'map_ins_T_315500_234500_NE.json',\n",
       " 'map_ins_T_315500_234500_NW.json']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAPTOINST_META_LIST"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc647e2225573627561fd5069e9d88ac514047f3a8797e32712e149d4deab2d3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
