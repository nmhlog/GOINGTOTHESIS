{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import json\n",
    "import glob\n",
    "from numpyencoder import NumpyEncoder\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import open3d\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 89.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/nmhlog/Naufal Disk/Dataset/T_315500_233500_NE_T_315500_234000_SE/meta_data.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# for i in tqdm.tqdm(glob.glob(\"/media/nmhlog/Naufal Disk/Dataset/T_315500_233500_NE_T_315500_234000_SE/**.json\")):\n",
    "#     print(i)\n",
    "#     shutil.copyfile(i, \"/\".join(i.split(\"/\")[4:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38038/38038 [1:02:02<00:00, 10.22it/s] \n"
     ]
    }
   ],
   "source": [
    "# for i in tqdm.tqdm(glob.glob(\"/media/nmhlog/Naufal Disk/Dataset/**/**.txt\")):\n",
    "#     shutil.copyfile(i, \"/\".join(i.split(\"/\")[4:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 57.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# for i in tqdm.tqdm(glob.glob(\"/media/nmhlog/Naufal Disk/Dataset/**/**.json\")):\n",
    "#     shutil.copyfile(i, \"/\".join(i.split(\"/\")[4:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 89.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# for i in tqdm.tqdm(glob.glob(\"/media/nmhlog/Naufal Disk/Dataset/T_316000_233500_NE_T_316000_233500_SW/**.txt\")):\n",
    "#     shutil.copyfile(i, \"/\".join(i.split(\"/\")[4:]))\n",
    "# for i in tqdm.tqdm(glob.glob(\"/media/nmhlog/Naufal Disk/Dataset/T_316000_233500_NE_T_316000_233500_SW/**.json\")):\n",
    "#     shutil.copyfile(i, \"/\".join(i.split(\"/\")[4:]))\n",
    "# for i in glob.glob(f\"{PATH}/**/b**.txt\"):\n",
    "#     os.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"Dataset\"\n",
    "METADATA= glob.glob(f\"{PATH}/**/**.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = [i.split(\"/\")[1] for i in glob.glob(f\"{PATH}/**/\")]\n",
    "existing_metadata = [i.split(\"/\")[1] for i in METADATA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_folder in folder_name:\n",
    "    if n_folder not in existing_metadata:\n",
    "        print(n_folder) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_meta_data(file_name,dict_file):\n",
    "    with open(f'{file_name}.json', 'w') as fp:\n",
    "        json.dump(dict_file, fp,  indent=4,cls=NumpyEncoder)\n",
    "\n",
    "def read_meta_data (path):\n",
    "    with open(path, 'r') as j:\n",
    "        contents = json.loads(j.read())\n",
    "        return contents\n",
    "        \n",
    "def del_data(files_name,folder_name=\"Dataset\"):\n",
    "    try:\n",
    "        for d in files_name:\n",
    "            os.remove(f\"{folder_name}/{d}\")\n",
    "    except:\n",
    "        os.remove(f\"{folder_name}/{files_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dataset/T_315500_234500_SW/meta_data.json',\n",
       " 'Dataset/T_316000_233500_NE_T_316000_233500_SW/meta_data.json',\n",
       " 'Dataset/T_316000_233500_NW/meta_data.json',\n",
       " 'Dataset/T_316000_233500_SE/meta_data.json',\n",
       " 'Dataset/T_316000_234000_NE/meta_data.json',\n",
       " 'Dataset/T_316000_234000_NW/meta_data.json',\n",
       " 'Dataset/T_316000_234000_SE/meta_data.json',\n",
       " 'Dataset/T_316000_234000_SW/meta_data.json',\n",
       " 'Dataset/T_316500_234000_SW_T_316500_233500_NW/meta_data.json']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METADATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA = METADATA[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    1    2 ... 1600 1601 1602]\n",
      "[   0    1    2 ... 3038 3039 3040]\n",
      "[   0    1    2 ... 2656 2657 2658]\n",
      "[   0    1    2 ... 1397 1398 1399]\n",
      "[   0    1    2 ... 4098 4099 4100]\n",
      "[   0    1    2 ... 2544 2545 2546]\n",
      "[   0    1    2 ... 1693 1694 1695]\n",
      "[   0    1    2 ... 3741 3742 3743]\n",
      "[   0    1    2 ... 2904 2905 2906]\n",
      "[   0    1    2 ... 2907 2908 2909]\n",
      "[   0    1    2 ... 2734 2735 2736]\n"
     ]
    }
   ],
   "source": [
    "b_data = [i.split(\"/\")[1] for i in METADATA]\n",
    "for b_d in b_data :\n",
    "    buff = glob.glob(f\"{PATH}/{b_d}/**.txt\")\n",
    "    data = np.array([i.split(\"/\") for i in buff if \"building\" in i or \"buidling\" in i])\n",
    "    new= data[:,-1].copy()\n",
    "    new = [i.split(\"_\") for i in new]\n",
    "    for i,_ in enumerate(new):\n",
    "        if len(new[i])==2:\n",
    "            new[i][1] = new[i][1].split(\".\")\n",
    "            new[i][1][0]= new[i][1][0].zfill(3)\n",
    "            new[i][1]= \".\".join(new[i][1])\n",
    "        else:\n",
    "            new[i][1]=new[i][1].zfill(3)\n",
    "        new[i]= \"_\".join(new[i])\n",
    "    index = np.where(data[:,-1]!=new)[0]\n",
    "    if index.size != 0:\n",
    "        old_data= data[:,-1]\n",
    "        METADATA_buff = read_meta_data(f'{PATH}/{b_d}/meta_data.json')\n",
    "        OLD_METADATA_buff = METADATA_buff.copy()\n",
    "        for i in index:\n",
    "            try:\n",
    "                data_edited= METADATA_buff.pop(old_data[i][:-4])\n",
    "            except:\n",
    "                continue\n",
    "            list_new_data=new[i].split(\"/\")\n",
    "            METADATA_buff[list_new_data[-1][:-4]]={\n",
    "                                                    \"path\":f\"{b_d}/{new[i]}\",\n",
    "                                                    \"jumlah_point\":data_edited[\"jumlah_point\"]\n",
    "                                                    }\n",
    "\n",
    "        print(index)\n",
    "        save_meta_data(f'{PATH}/{b_d}/meta_data',METADATA_buff)\n",
    "        for idx in index:\n",
    "            os.rename(f\"{PATH}/{b_d}/{old_data[idx]}\",f\"{PATH}/{b_d}/{new[idx]}\")\n",
    "        #     # print()\n",
    "        #     # print(old_data)\n",
    "# METADATA= glob.glob(f\"{PATH}/**/**.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.71it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_list_per_instace(str_name ,file_list,Area,dict_file):\n",
    "    result_dict = {}\n",
    "    id_ns = np.unique(np.array([b.split(\"_\")[1] for b in dict_file],dtype=\"int16\"))\n",
    "    zf = 3 #if Area ==  \"T_316000_233500_NE_T_316000_233500_SW\" or Area==\"T_315500_234500_SE\" else 2\n",
    "    for idn in id_ns :\n",
    "        r = re.compile(f\"{str_name}_{str(idn).zfill(zf)}\")\n",
    "        result_dict[f\"{str_name}_{str(idn).zfill(zf)}\"] = list(filter(r.match, file_list)) \n",
    "    return result_dict\n",
    "\n",
    "for i,_ in tqdm.tqdm(enumerate(METADATA),total=len(METADATA)):\n",
    "    Area = METADATA[i].split(\"/\")[1]\n",
    "    md = read_meta_data(METADATA[i])\n",
    "    file_names = md.keys()\n",
    "    building,ground,undefined,vegetation=[],[],[],[]\n",
    "    label = set([i.split(\"_\")[0] for i in file_names])\n",
    "    for fn in file_names:\n",
    "        if 'building' in fn.lower() or \"buidling\" in fn.lower():\n",
    "            building.append(fn)\n",
    "        if 'ground' in fn.lower():\n",
    "            ground.append(fn)\n",
    "        if 'undefined' in fn.lower() or \"undifined\" in fn.lower() or \"undefined\" in fn.lower() or 'Undefined' in fn.lower():\n",
    "            undefined.append(fn)\n",
    "        if 'vegetation'in fn.lower():\n",
    "            vegetation.append(fn)\n",
    "    dict_area={\n",
    "        \"building\":get_list_per_instace(str_name = \"building\",file_list = building,dict_file=building,Area=Area),\n",
    "        \"vegetation\":vegetation,\n",
    "        \"ground\":ground,\n",
    "        \"undefined\":undefined\n",
    "        }\n",
    "    save_meta_data(f\"map_ins_{Area}\",dict_area)\n",
    "\n",
    "def maping_meta_data(class_name,map_data,meta_data):\n",
    "    inst_id = {}\n",
    "    try:\n",
    "        for count,key in enumerate(map_data[class_name]):\n",
    "            try:\n",
    "                k_split = key.split(\"_\")\n",
    "                if len(k_split)==3:\n",
    "                    _,sub_class,key_id = k_split\n",
    "                else :\n",
    "                    sub_class,key_id[:-4] = k_split\n",
    "            except:\n",
    "                sub_class,key_id=key,0\n",
    "                \n",
    "            inst_id[str(count).zfill(3)]={\n",
    "                \"total_point\": meta_data[key][\"jumlah_point\"],\n",
    "                \"path\": meta_data[key][\"path\"],\n",
    "                \"sub_class\": sub_class,\n",
    "                \"id\": str(int(key_id)-1).zfill(3)\n",
    "                }\n",
    "    except Exception as e:\n",
    "        print(class_name)\n",
    "        print(meta_data[class_name])\n",
    "        inst_id[\"000\"]={\n",
    "            \"total_point\": meta_data[class_name][\"jumlah_point\"],\n",
    "            \"path\": meta_data[class_name][\"path\"],\n",
    "            \"sub_class\": \"000\",\n",
    "            \"id\": \"000\"\n",
    "            }\n",
    "    return inst_id\n",
    "    \n",
    "def map_to_inst(map_data_json,meta_data_json):\n",
    "    map_data = read_meta_data(map_data_json)\n",
    "    meta_data= read_meta_data(meta_data_json)\n",
    "    for k in map_data.keys():\n",
    "        if k == \"building\":\n",
    "            inst_id_building = {}\n",
    "            list_build_id= list(map_data[k].keys())\n",
    "            for id_b in list_build_id:\n",
    "                id_ins = str(int(id_b.split(\"_\")[-1])-1).zfill(3)\n",
    "                total_point = sum([meta_data[i][\"jumlah_point\"] for i in map_data[list(map_data.keys())[0]][id_b]])\n",
    "                list_all_txt = [meta_data[i][\"path\"] for i in map_data[list(map_data.keys())[0]][id_b]]\n",
    "                inst_id_building[id_ins]={\n",
    "                    \"total_point\":total_point,\n",
    "                    \"path\":list_all_txt\n",
    "                    }\n",
    "        elif k==\"vegetation\" :\n",
    "            inst_id_vegetation = maping_meta_data(\"vegetation\",map_data,meta_data)\n",
    "        elif k==\"ground\" :\n",
    "            inst_id_ground = maping_meta_data(\"ground\",map_data,meta_data)\n",
    "        else :\n",
    "            inst_id_undefined = maping_meta_data(\"undefined\",map_data,meta_data)\n",
    "    return [inst_id_building,inst_id_vegetation,inst_id_ground,inst_id_undefined]\n",
    "\n",
    "for i,_ in enumerate(METADATA):\n",
    "    area=METADATA[i].split(\"/\")[1]\n",
    "    ins = map_to_inst(f\"map_ins_{area}.json\",METADATA[i])\n",
    "    save_meta_data(f\"map_ins_{area}\",{\"building\":ins[0],\"vegetation\":ins[1],\"ground\":ins[2],'undefined':ins[3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [42:57<00:00, 198.25s/it]\n"
     ]
    }
   ],
   "source": [
    "MAPTOINST_META_LIST= glob.glob(\"map_**\")\n",
    "def get_combining_building_data(map_data_json = MAPTOINST_META_LIST,buff_folder=\"Dataset\") :   \n",
    "    for i,meta_name in tqdm.tqdm(enumerate(map_data_json),total=13):\n",
    "        map_data = read_meta_data(map_data_json[i])[\"building\"]\n",
    "        buff_folder = f\"all_data_building/{map_data_json[i][8:-5]}\"\n",
    "        os.makedirs(buff_folder,exist_ok=True)\n",
    "        map_data_id= list(map_data.keys())\n",
    "        for mdi in map_data_id:\n",
    "            list_data_txt = map_data[mdi][\"path\"]\n",
    "            # Start from init = 0 and append until last\n",
    "            all_data = np.loadtxt(f\"Dataset/{list_data_txt[0]}\")[:,:6]\n",
    "            for idx in range(1,len(list_data_txt)):\n",
    "                try:\n",
    "                    buff = np.loadtxt(f\"Dataset/{list_data_txt[idx]}\")[:,:6]\n",
    "                    all_data = np.append(all_data,buff,axis=0)\n",
    "                except :\n",
    "                    buff = np.loadtxt(f\"Dataset/{list_data_txt[idx]}\")[:6]\n",
    "                    all_data = np.append(all_data,[buff],axis=0)\n",
    "            del_data(list_data_txt)\n",
    "            if \"T_316000_233500_NE_T_316000_233500_SW\" in meta_name:\n",
    "                data_building_index = np.where(all_data[:,0]<318000)\n",
    "                data_bulding_sub_building_index = np.where(all_data[:,0]>318000)\n",
    "                data_building = all_data[data_building_index]\n",
    "                data_bulding_sub_building = all_data[data_bulding_sub_building_index]\n",
    "                data_bulding_sub_building[:,0] = data_bulding_sub_building[:,0]-2000\n",
    "                data_bulding_sub_building[:,1] = data_bulding_sub_building[:,1]-3619\n",
    "                data_bulding_sub_building[:,2] = data_bulding_sub_building[:,2]+40\n",
    "                all_data = np.append(data_building,data_bulding_sub_building,axis=0)\n",
    "            save_txt_loc = f\"{buff_folder}/building_{mdi}.txt\"\n",
    "            map_data[mdi][\"path\"] = save_txt_loc\n",
    "            np.savetxt(save_txt_loc,all_data,fmt='%.8f %.8f %.8f %d %d %d')\n",
    "        old_map = read_meta_data(map_data_json[i])\n",
    "        old_map[\"building\"]= map_data\n",
    "        save_meta_data(f\"{map_data_json[i][8:-5]}\",old_map)\n",
    "        os.remove(f\"{map_data_json[i]}\")\n",
    "get_combining_building_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "np.savetxt(\"all_data_building/T_315500_234500_SW/building_065.txt\",all_data,fmt='%.8f %.8f %.8f %d %d %d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/T_315500_234500_SW/building_66_roof_01.txt\n",
      "Dataset/T_315500_234500_SW/building_66_window_01.txt\n",
      "Dataset/T_315500_234500_SW/building_66_window_02.txt\n",
      "Dataset/T_315500_234500_SW/building_66_window_03.txt\n",
      "Dataset/T_315500_234500_SW/building_66_window_04.txt\n",
      "Dataset/T_315500_234500_SW/building_66_window_05.txt\n",
      "Dataset/T_315500_234500_SW/building_66_window_06.txt\n",
      "Dataset/T_315500_234500_SW/building_66_window_07.txt\n",
      "Dataset/T_315500_234500_SW/building_66_window_08.txt\n",
      "Dataset/T_315500_234500_SW/building_66_window_09.txt\n",
      "Dataset/T_315500_234500_SW/building_66_window_10.txt\n",
      "Dataset/T_315500_234500_SW/building_66_window_11.txt\n",
      "Dataset/T_315500_234500_SW/building_66_window_12.txt\n",
      "Dataset/T_315500_234500_SW/building_66_window_13.txt\n",
      "Dataset/T_315500_234500_SW/building_66_window_14.txt\n",
      "Dataset/T_315500_234500_SW/building_66_window_15.txt\n",
      "Dataset/T_315500_234500_SW/building_66_window_16.txt\n",
      "Dataset/T_315500_234500_SW/building_66_window_17.txt\n"
     ]
    }
   ],
   "source": [
    "# list_data_txt = map_data[mdi][\"path\"]\n",
    "data = glob.glob(\"Dataset/T_315500_234500_SW/b**\")\n",
    "all_data = np.loadtxt(data[0])[:,:6]\n",
    "\n",
    "for i in data[1:]:\n",
    "    # for idx in range(1,len(list_data_txt)):\n",
    "    print(i)\n",
    "    # try:\n",
    "    buff = np.loadtxt(i)[:,:6]\n",
    "    all_data = np.append(all_data,buff,axis=0)\n",
    "    # except :\n",
    "    #     buff = np.loadtxt(i)[:6]\n",
    "    #     all_data = np.append(all_data,[buff],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50250"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40451"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.loadtxt(\"Dataset/T_315500_234500_SW/building_66_roof_01.txt\")[:,:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dataset/T_315500_234500_SW/building_66_window_17.txt'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc647e2225573627561fd5069e9d88ac514047f3a8797e32712e149d4deab2d3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
